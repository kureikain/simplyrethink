-# 4. Modifying data

We knew how to fetch the data. But a database is useless without ability of
writing data. In this chapter, we will learn about modifiying data. We will
address database level command, table level command, and then document command
in this chapter.

# Database

All commands on database levels start at the top namespace `r`. They are
usually the first command right after `r`. Let's start our journey by creating
a database. Remember, we need a database to hold everything, right?

## Create

Very simple. With examples, you will get it easily.

    //Create database
    r.dbCreate("db1")
    #=>
    {
    "config_changes": [
    {
    "new_val": {
    "id":  "5e4a85fa-d867-4a93-aa01-2d08ed6f0b14" ,
    "name":  "db1"
    } ,
    "old_val": null
    }
    ] ,
    "dbs_created": 1
    }

If creating succesfully, we get back the object with `created` is always *1*.
**config_changes** will have **new_val** field is the database's config value.
**old_val** is always *null* becase this is a new database. **config value**
is the configuration for an individual database or table. What is the 
configuration? Usually, when we create any object in RethinkDB (a database, 
a table) we can pass a list of option to that creating command. That option
has to be stored somewhere and we should have ability to read it back.
For a database, configuration is just its name and its id. That's why you see
the id and name are returned in above query. We will learn more about this
configuration very quick in this chapter.

We can confirm by listing what we have:

    r.dbList()
    #=>
    [
    "foodb" ,
    "rethinkdb" ,
    "superheroes" ,
    "test"
    ]

Notice a special database call ***rethinkdb***? This is a special database that
is created by RethinkDB to hold meta data, configuration. It's very similar to
**mysql** database in a MySQL server. Remember the configuration of `dbCreate`
function? Those configuration is stored in table `db_config` inside this
database `rethinkdb`.

## Drop

So we got the default test and db1 is what we just have. Since we don't use db1,
let's delete it to keep our database clean.

    r.dbDrop('db1')
    #=>
    {
    "config_changes": [
    {
    "new_val": null ,
    "old_val": {
    "id":  "5e4a85fa-d867-4a93-aa01-2d08ed6f0b14" ,
    "name":  "db1"
    }
    }
    ] ,
    "dbs_dropped": 1 ,
    "tables_dropped": 0
    }

Very similar with `dbCreate` but in an opposite way. Now `new_val` is **null**
because the database is no longer existed. **old_val** is the id and name of
old database, or the old configuration of database. 

# Table

Tables have to sit inside a database, therefore, all table commands have to call
on a database. When you don't explicit specify a database to run on with `r.db`,
the current database will be the base for table manipulation.

## Create

The syntax to create a table is

    db.tableCreate(tableName[, options])

The second parameter is optional. This is what we consider configuration for
a table. It's similar to database configuration. But table configuration is
much richer. Some important ones are:

*primaryKey

: the name of primary key. Default name of primary key is **id**. The value of
**id** field will always be indexed automatically and using as primary key.
Using this option, you can change that default behavior such as using **uuid**
as default primary key. When a new document is inserted, RethinkDB will fetch
value of **uuid** field to create index instead of field **id**

*durability

: accept value of *soft* or *hard*. *soft* means the writes will be
acknowledged by server immdediately, and data will be flushed to disk in the
background. If that flushing fail, we may not know. The default behaviour is
to acknowledge after data is written to disk. That means *hard*. It's default
because it's much safety.
When we don't need the data to be consitent, such as writing a cache, or an
unimportant log, we should set durability to soft to speed up the writing.
However, for any important, serious data, keep it default.

RethinkDB stores configuration of each table in a special table call
**table_config** inside database **rethinkdb**.

Let's try to create a table.

    r.db("foodb").tableCreate("t1",  {primaryKey: 'uuid'})

## List table

To list what table we have inside a database, we use `tableList` command. It's
similar to **SHOW TABLE** in MySQL.

    r.db("foodb").tableList()
    //=>
    [
    "compound_synonyms" ,
    "compounds" ,
    "compounds_flavors" ,
    "compounds_foods" ,
    "compounds_health_effects" ,
    "flavors" ,
    "foods" ,
    "health_effects" ,
    "t1" ,
    "users"
    ]

## Drop table

To get rid of the table, use `tableDrop` command. 

    r.db("foodb").tableDrop("t1")
    //=>
    {
        "config_changes": [{
            "new_val": null,
            "old_val": {
                "db": "foodb",
                "durability": "hard",
                "id": "d20fe79e-9e90-4625-95f7-c9e1953bf773",
                "name": "t1",
                "primary_key": "id",
                "shards": [{
                    "primary_replica": "SimplyRethinkDB",
                    "replicas": [
                        "SimplyRethinkDB"
                    ]
                }],
                "write_acks": "majority"
            }
        }],
        "tables_dropped": 1
    }

Very similar to `dbDrop`, we also have **config_changes**. **new_val** always
null because the table is gone now. **old_val** is the configuration of removed
table. Table configuration is usually what we passed in when we create it with
**tableCreate**.

We see that some **db** and **table** command returns **config_changes**. Let's
discover where those configs are stored.

# System table

Usually a database server have to keep some meta data, or configuration
information somewhere else. In case of RethinkDB, it stores those data in
**rethinkdb** data. Let's discover this database: 

    r.db("rethinkdb").tableList()
    [
    "cluster_config" ,
    "current_issues" ,
    "db_config" ,
    "jobs" ,
    "logs" ,
    "server_config" ,
    "server_status" ,
    "stats" ,
    "table_config" ,
    "table_status"
    ]

The name of each table should suggest what it contains. Let's inspect **server_config**

    r.db("rethinkdb").table("server_config")
    //=>
    {
    "cache_size_mb":  "auto" ,
    "id":  "fdc5dade-2f0c-498f-8c4b-59ad0d976471" ,
    "name":  "Vinh_local_u27" ,
    "tags": [
    "default"
    ]
    }

Let's change our server name:

  r.db("rethinkdb").table("server_config")
    .get("fdc5dade-2f0c-498f-8c4b-59ad0d976471")
    .update({name: "SimplyRethinkDB"})

You will notice that the Admin UI will change the server name:

![Server name changes to SimplyRethinkDB](images/chapter4/servername_ui.png)

By modifying those table, we change the configuration of our server. We can get
RethinkDB version by querying `server_status`.

    r.db("rethinkdb").table("server_status")("process")("version")

In other words, those system table refelects information related to how the
system operates. We can query to fetch or modify system information.

We can get configuration that we set when creating table with `tableCreate` of
any table:

    r.db("rethinkdb").table("table_config")
    //=>
    {
    "db":  "foodb" ,
    "durability":  "hard" ,
    "id":  "2e41fc0b-ea5e-4460-bd3b-5d33a5ec49af" ,
    "name":  "health_effects" ,
    "primary_key":  "id" ,
    "shards": [
    {
    "primary_replica":  "SimplyRethinkDB" ,
    "replicas": [
    "SimplyRethinkDB"
    ]
    }
    ] ,
    "write_acks":  "majority"
    } {
    "db":  "foodb" ,
    "durability":  "hard" ,
    "id":  "3fbf59ad-35df-445c-9fa9-be19071d38d7" ,
    "name":  "compounds_flavors" ,
    "primary_key":  "id" ,
    "shards": [
    {
    "primary_replica":  "SimplyRethinkDB" ,
    "replicas": [
    "SimplyRethinkDB"
    ]
    }
    ] ,
    "write_acks":  "majority"
    }

Looking at the above result, we can see that table `health_effects` of database
`foodb` has **primary_key** is **id**, and **write_acks** is **majority**.

You can have more fun and some deep understanding under the hood by inspecting
those tables.

# Document

After creating database and creating table, we can start inserting document into
table.

## Insert

As you can guess, we will start from the database, chain the table, and use
`insert` command to insert a document into the table. Eg

    // Let create a fake user
    r.db("foodb").table("users")
      .insert({id: "user-foo1", name: "foo", age: 12})
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 1 ,
      "replaced": 0 ,
      "skipped": 0 ,
      "unchanged": 0
    }

Here, we set our own primary key for **id** field. If we don't set it. RethinkDB
will generate it and return for us via the return object.

    r.db("foodb").table("users")
      .insert({name: "foo", age: 12})
    //=>
    {
    "deleted": 0 ,
    "errors": 0 ,
    "generated_keys": [
    "b7e23aa4-e7d8-4d3e-9020-4c5b1ec413c6"
    ] ,
    "inserted": 1 ,
    "replaced": 0 ,
    "skipped": 0 ,
    "unchanged": 0
    }

The return object contains the following attributes: 

inserted
: the number of documents that were succesfully inserted.

replaced
: the number of documents that were updated when upsert is used.

unchanged
: the number of documents that would have been modified, except that the new value was the same as the old value when doing an upsert.

errors
: the number of errors encountered while performing the insert.

first_error
: If errors were encountered, contains the text of the first error.

deleted, skipped
: 0 for an insert operation.

generated_keys
: a list of generated primary keys in case the primary keys for some documents were missing (capped to 100000).

warnings
: if the field generated_keys is truncated, you will get the warning: "Too many generated keys (<X>), array truncated to 100000.".
old_val
: if returnVals is set to true, contains null.
new_val
: if returnVals is set to true, contains the inserted/updated document.

Notice the generated_keys. If we insert a document without set a value for
primary key, whose field name is `*id*` by default, RethinkDB will generate an
UUID[^uuid] for it and use it as value of `*id*` field.

[^uuid]: http://en.wikipedia.org/wiki/Universally_unique_identifier

With our example, the primary key is **b7e23aa4-e7d8-4d3e-9020-4c5b1ec413c6**.
We can retreive back the document again.

    r.db("foodb").table("users")
      .get('b7e23aa4-e7d8-4d3e-9020-4c5b1ec413c6')
    //=>
    {
      "age": 12 ,
      "id":  "b7e23aa4-e7d8-4d3e-9020-4c5b1ec413c6" ,
      "name":  "foo"
    }

### Multi insert

If we have a large array of data, we don't have to insert one by one, we can
pass the whole array to insert to do batch insert, which is much efficient than
update one by one

Let's play with it a bit. Create a test table on our **test** db.

    r.db("test").tableCreate("users", {primaryKey: 'myid'})

We also use primary key as field **myid** insetead of usind **id**.
We can insert multiple data at a time. Some document has **myid**,
some of them don't have. We will see how RethinkDB generate primary key for
those documents:

    r.db("test").table("users")
      .insert([
        {
          myid: 1,
          name: 'Hydra'
        },
        {
          name: 'Pluto'
        },
        {
          name: 'Styx',
          myid: 'abcxyz'
        }
      ])
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "generated_keys": [
      "8c3a1d6c-2b7b-4a4f-91dc-d6855c5aed15"
      ] ,
      "inserted": 3 ,
      "replaced": 0 ,
      "skipped": 0 ,
      "unchanged": 0
    }

Here, **generated_keys** contains a single element **8c3a1d6c-2b7b-4a4f-91dc-d6855c5aed15**.
The reason is only second inserted element `{name: 'Pluto'}` doesn't have
`myid` field, the other two we set manually so RethinkDB just use it.

Let's verify:

    r.db("test").table("users")
    //=>
    {
        "myid": 1,
        "name": "Hydra"
    } {
        "myid": "abcxyz",
        "name": "Styx"
    } {
        "myid": "8c3a1d6c-2b7b-4a4f-91dc-d6855c5aed15",
        "name": "Pluto"
    }

Yay, how cool is that? We used a custom primary key, we insert multiple document
at a time and RethinkDb assign a primary key for it.

Let's see if `myid` field is really use as primary index. We can call `get`
command because `get` operator on primary key:

    r.db("test").table("users")
      .get('abcxyz')
    //=>
    {
      "myid":  "abcxyz" ,
      "name":  "Styx"
    }

### Effect of **durability**

Let's see the difference of **durability**. We will insert a big document.

First, I will create a temporary table
    r.tableCreate("git")

Then insert data, to have a big amount of data. I use `http`, which is a
command that fetch external JSON data, which is really useful to deal with
external API. We can treat the result of `r.http` as a normal JSON document.
`r.http` takes care of fetching data via HTTP and turn document into valid
JSON.

    r.table('git').insert(
      r.http('https://api.github.com/repos/rethinkdb/rethinkdb/stargazers')),
      {durability:soft}
    )
    Executed in 773ms. 1 row returned

Now, if I turn on **durability**.

    r.table('git').insert(
      r.http('https://api.github.com/repos/rethinkdb/rethinkdb/stargazers')),
      {durability:soft}
    )
    Executed in 1.18s. 1 row returned

So it's slower because it takes time to write to hard drive. You may not see
the effect if you have a very fast hard drive or SSD. I tried it on an external
spin drive :).

## Conflict when inserting document

What happens when we insert a document with same primary keys?

    r.tableCreate('t')
    r.table('t').insert({id: 1, name: 'Vinh'})
    r.table('t').insert({id: 1, name: 'Vinh'})

On the second time, I got this error:

    {
        "deleted": 0,
        "errors": 1,
        "first_error": "Duplicate primary key `id`:\n{\n\t\"id\":\t1,\n\t\"name\":\t\"Vinh\"\n}\n{\n\t\"id\":\t1,\n\t\"name\":\t\"Vinh\"\n}",
        "inserted": 0,
        "replaced": 0,
        "skipped": 0,
        "unchanged": 0
    }

So by default, the behaviour is an error is throw. However, we can change that
behaviour. RethinkDB supports this via a `conflict` field in option of insert.
It accepts 3 values:

 

## Update

To make it easier, you can think of updating like selecting data, then change
their value. We chain `update` method from a selection range to update its data.
With that being said, we can update one or many documents at a time. Similar to
MySQL, we can update a full table, or update only rows that sastify a **WHERE**
condition.

Think of modification is like a transform process where you get a list of
document(one or many), then transform by adding fields, rewrite value for some fields.
By that definition, it doesn't matter if you update one document, or many 
document.As long as you have an array, or a stream of data, you can update them
all.

For example, to update an attribute for a single element

    // Let update age and add a new field
    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        age: 13,
        gender: "f"
      })
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 0 ,
      "replaced": 1 ,
      "skipped": 0 ,
      "unchanged": 0
    }

RethinkDB returns an object for the updating result. We can look into `replaced`
field to see if the data is actually updated. If we re-run the above command,
nothing is replaced and we will got 1 `unchanged`.

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        age: 13,
        gender: "f"
      })
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 0 ,
      "replaced": 0 ,
      "skipped": 0 ,
      "unchanged": 1
    }

That's just how awesome RethinkDB is. All query result is very verbose. And
easy to understand.

In above example, you can see when we update **age**, we also add a new field
**gender**. The updating process can be understand as a merge process of
return values from update function or update expression into current existed
document. Let's verify if **gender** field are really there:

    r.db("foodb").table("users")
      .get("user-foo1")
    //=>
    {
      "age": 13 ,
      "gender":  "f" ,
      "id":  "user-foo1" ,
      "name":  "foo"
    }

We can also update nested field. Let's add an address field:

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        "address" : {
        country: "USA",
        state: "CA",
        city: "Cuppertino",
        street: "Infinite Loop",
        number: "1",
        ste: "1205"
      }
      })
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 0 ,
      "replaced": 1 ,
      "skipped": 0 ,
      "unchanged": 0
    }

**replaced** is 1, that means we update succesfully. Now, let's say I moved,
I can change the address:

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        "address" : {
        ste: 880,
        number: 11
      }
      })
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 0 ,
      "replaced": 1 ,
      "skipped": 0 ,
      "unchanged": 0
    }

Here, we are updating field `ste` of field `address`.

    r.db("foodb").table("users")
      .get("user-foo1")
    //=>
    {
      "address": {
      "city":  "Cuppertino" ,
      "country":  "USA" ,
      "number":  11 ,
      "state":  "CA" ,
      "ste":  880 ,
      "street":  "Infinite Loop"
      } ,
      "age": 13 ,
      "gender":  "f" ,
      "id":  "user-foo1" ,
      "name":  "foo"
    }

The value of the updated field, as you can see in above example, is a single
value.

#### Update option

`update` command receive some options that you can pass. In JavaScript, you can
pass option object as second parameter. In Ruby, you can use optional keyword.
Such as in JavaScript:

    r.table("posts").get(1).update({
        num_comments: r.js("Math.floor(Math.random()*100)")
    }, {
        nonAtomic: true
    })

Here, `{nonAtomic: true}` is out option. In Ruby, it's more elegant due:

    r.table("posts").get(1).update({
        :num_comments => r.js("Math.floor(Math.random()*100)")
    }, :non_atomic => true)

We have 3 options parameters:

  * *durability*: possible values are hard and soft. You already know what it does. However, setting it here override durability default of tables
  * *non_atomic*: you should also know what it does. If not, coming back chapter2.

So we know the option. Let's move on. In this section, we learn how to update
value for a field, update nested value. What if the field contains an array?
How can we append new element. Or how to
update value which is the result of other ReQL command. Let's move to next
section.

### Update data for complex field

First, we see that an user can have many address. Right now, our *address*
field is a single object. Let's make it an array so it can accepts many
address. Using previous array that we created.

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        address: [r.row("address")]
      })

Here we are using `r.row`, it allows access to current document we are
referencing to. We turn **address** into an array by wrap it in `[]`
to turn it into array.

Now, we got `address` is an array with a single address, How do we add more
data into that array. The simple form, we can pass an value to the `update`.
First, we get the value of current address, append a new element using what
our language offer. In JavaScript code, we may have:

    addresses = r.db("foodb").table("users")
              .get("user-foo1")("address")
    addresses.push(new_add_ress)
    r.db("foodb").table("users")
      .get("user-foo1")
      .update({address: addresses})

That's work but it's very inefficent. For example, if we have to append a new
element to an array for 1000 users. We have to fetch the data, change it,
update by sending new array again.

Also a more important issue is updating lock. When we are retriving data, alter
it
on client side, push back data to database. During the time since we get the
data and push it back. The server may changes the data and we didn't aware of
it in first query to retrieve data, now when we push back, we override that new
changes. Image this, we have 2 admins on the sites, who are trying to edit an
user at the same time to update user's address.

First, admin 1 retrieve data, add new address B and push it back. For whatever
reason, admin 2 retrieve data, right after admin 1 retrieve
data, then add new address B and update it, but before admin 1 push it back. 
So when admin 1 pushs data back, the changes admin 2 created is override.

It's would be great if we can move the logic into RethinkDB and let's
RethinkDB handles lock for it. Just like how in SQL we can do:

    UPDATE TABLE user SET login=login+1

We tell MySQL to increment value of login by 1 instead of doing that outself.
Luckily, we have that in RethinkDB. Some of them falls under **Document
manipulation**[^Documentmanipulation] section on RethinkDB docs. They allow us
do some logic to the document.

[^Documentmanipulation]: http://www.rethinkdb.com/api/ruby/pluck/

Our example above can be written using `append`. **append** command add a new
element to array.

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        address: r.row("address")
        .append({country: "Vietnam", 
                    city: "Hue", 
                          street: "Tran Phu",
                    number: "131"})
      })
    //=>
    {
      "deleted": 0 ,
      "errors": 0 ,
      "inserted": 0 ,
      "replaced": 1 ,
      "skipped": 0 ,
      "unchanged": 0
    }

What if an user has not **address** field on it yet? Well, an error will be
thrown out. let's try:

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        another_address_field: r.row("another_address_field")
        .append({country: "Vietnamnam", 
                    city: "Hue", 
                    street: "Tran Phu",
                    number: "131"})
      })
    //=>
    {
        "deleted": 0,
        "errors": 1,
        "first_error": "No attribute `another_address_field` in object: {
            "address": [{
                "city": "Cuppertino",
                "country": "USA",
                "number": 11,
                "state": "CA",
                "ste": 880,
                "street": "Infinite Loop"
            }, {
                "city": "Hue",
                "country": "Vietnam",
                "number": "131",
                "street": "Tran Phu"
            }],
            "age": 13,
            "gender": "f",
            "id": "user-foo1",
            "name": "foo"
        }
        " ,
        "inserted": 0,
        "replaced": 0,
        "skipped": 0,
        "unchanged": 0
    }

To avoid that, we have tell RethinkDB what value should be used when that
field doesn't exist. Such as for an array, we can consider that value is
an empty array. For a string we can consider that value is an empty string.
For a positive number, that can be zero.

We use `defaul(default_value)` command for this purpose. Let's try it:

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({
        another_address_field: 
                r.row("another_address_fieldess_field")
                  .default([])
                  .append({country: "Vietnamnamam", 
                    city: "Hue", 
                    street: "Tran Phu",
                    number: "131"})
      })
    //=>
    {
    "deleted": 0 ,
    "errors": 0 ,
    "inserted": 0 ,
    "replaced": 1 ,
    "skipped": 0 ,
    "unchanged": 0
    }

When we call command `default` on a value or a sequence, it will try to evalute
to the default value in case of non-existence error for the value. We can verify
address again:

    r.db("foodb").table("users")
      .get("user-foo1")
    //=>
    {
        "address": [{
            "city": "Cuppertino",
            "country": "USA",
            "number": 11,
            "state": "CA",
            "ste": 880,
            "street": "Infinite Loop"
        }, {
            "city": "Hue",
            "country": "Vietnam",
            "number": "131",
            "street": "Tran Phu"
        }],
        "age": 13,
        "another_address_field": [],
        "gender": "f",
        "id": "user-foo1",
        "name": "foo"
    }

So we had `append` to add element at the end of array, we can also use
`prepend`. It adds a new element to an array but at the top.

Take another example, we want to count how many **like** an user has. We will
use a fiel call **like** and we increase it by 1 whenever someone like the user.

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({like: r.row("like").add(1)})

You notice that we have to use `add` command instead of writing like:

    like: r.row("like") +1

Because these expressions are evaluated on RethinkDB server, not on client. When I
first learn RethinkDB, somehow I didn't understand it. I'm probably dumb. I
keep thinking those run on client and it makes my life harder. So I'm trying
to remind this several time through the book just in case someone confuse
like me. In some language that support operator overloading, you may use 

    like: r.row("like") + 1

That's because the driver override **+** operator to make it easy to write
expression. They are still serialized to ReQL syntax by driver.

Now, with above **update* command, we got error, which is expeteced:

    {
        "deleted": 0,
        "errors": 1,
        "first_error": "No attribute `like` in object: {
            "address": [{
                "city": "Cuppertino",
                "country": "USA",
                "number": 11,
                "state": "CA",
                "ste": 880,
                "street": "Infinite Loop"
            }, {
                "city": "Hue",
                "country": "Vietnam",
                "number": "131",
                "street": "Tran Phu"
            }],
            "age": 13,
            "another_address_field": [{
                "city": "Hue",
                "country": "Vietnam",
                "number": "131",
                "street": "Tran Phu"
            }],
            "gender": "f",
            "id": "user-foo1",
            "name": "foo"
        }
        " ,
        "inserted": 0,
        "replaced": 0,
        "skipped": 0,
        "unchanged": 0
    }

We have to set a default value for it. Let's default to 0. 

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({like: r.row("like").default(0).add(1)})
    // Get back like
    r.db("foodb").table("users").get("user-foo1")("like")
    //=>
    1

`add` is not limited on numeric data, it works on **array**, **string** too.
I will leave that part for you as an exercise.

Now we know how to work with an array as value of a field. Let's dive into how
to work with an object as a value of a field. Considering that we have this:

    r.db("foodb").table('users')
      .get('user-foo1')
      .update({
        social: {twitter: "kureikain"}
      })
    //=>
    {

        "deleted": 0 ,
        "errors": 0 ,
        "inserted": 0 ,
        "replaced": 1 ,
        "skipped": 0 ,
        "unchanged": 0
    }

The `social` field is an object now. Now, the user enters his facebook username
so we we want to add a new field **facebook** to **social** field to denote the
**facebook** account of user. We can not use `append` or `add` on an object.
For object, we use `merge` to add or override a field.

    r.db("foodb").table('users')
      .get('user-foo1')
      .update({
        social: r.row('social').default({}).merge({facebook: "kureikain"})
      })
    //=>
    {

        "deleted": 0 ,
        "errors": 0 ,
        "inserted": 0 ,
        "replaced": 1 ,
        "skipped": 0 ,
        "unchanged": 0

    }

Same as `append`, we also have to set a default value to handle non-existence
error. Since we are working with an object, we set its default value to empty
object: `{}`. `merge` overide existed key with new value in the object you
are passing, or create new key from the passing object.

    r.db("foodb").table('users')
      .get('user-foo1')
      .update({
        social: r.row('social').default({}).merge({facebook: "kureikain2", twitter: "kureikain2"})
      })
    //=>
    {

        "deleted": 0 ,
        "errors": 0 ,
        "inserted": 0 ,
        "replaced": 1 ,
        "skipped": 0 ,
        "unchanged": 0

    }
    
    // Select it back
    r.db("foodb").table('users')
      .get('user-foo1')("social")
    //=>
    {
      "facebook":  "kureikain2" ,
      "twitter":  "kureikain2"
    }

Cool, so we know how to add new fields or override old fields. But do you
notice they when a field contains an object, they are actually nested field. 
So we can easily update use nested field knowledge before instead of using
`merge` command:

    r.db("foodb").table('users')
      .get('user-foo1')
      .update({
        social: {facebook: "kureikain3", github: "kureikain"}
      })

It's really up to you to use `merge` or the nested field style. I usually using
nested field style when doing simple update, and `merge` when I want to merge
the document to other result from other ReQL function. But that's just opinion.

### Update multiple documents

Instead of select a single document and update one by one, you can update a
bunch of documents by calling `update` on a table or a stream, a selection.
All are same like what we do above, but instead of applying update to a single
document, it updates all element in the stream.

    r.table.filter(r.row('Day').gt(1) && r.row('Day').lt(90))
      .update({quarter: 1})
    //=>
    {

        "deleted": 0 ,
        "errors": 0 ,
        "inserted": 0 ,
        "replaced": 55 ,
        "skipped": 0 ,
        "unchanged": 0
    }

### ReQL inside the updated object

As you notice, we not only pass value into an updated object, but also passing
ReQL into updated object. As long as it can be evaluated like we increased **like**
to 1 with:

    r.db("foodb").table("users")
      .get("user-foo1")
      .update({like: r.row("like").default(0).add(1)})

However, `r.row` has a limit. It cannot be called on nested query. Assume that
we have a **friends** table that we can create with below query:

    // First, creata table
    r.db("foodb").tableCreate('friends')

    // Insert some faked data
    r.db("foodb").table("friends")
      .insert([{
        friend1_id: '12063f5f-4289-4a4b-b668-0e4a90861575',
        friend2_id: 'user-foo1'
      },
      {
        friend1_id: '8d4bcd47-3f7f-4670-a31c-2b807e3f7caf',
        friend2_id: 'user-foo1'
      }])

So we can see that our user with id **user-foo1** has 2 friends. It will not
very efficient if we have to count this over and over. So we are going to count
this and update **users** table with a field **friend_count**.

To count, we can get a sequence of **friend2_id**, and `count` how many items
has same value as current user id, by passing a value to `count` function. When
we pass a value to `count`, it only counts the document equal to that value.
Here, We are trying use `r.row` to reference to current user.

    r.db('foodb').table('users')
      .get('user-foo1')
      .update({
        friend_counts: r.db('foodb').table('friends')('friend2_id').count(r.row('id'))
      })

If we run above query we will get this error


C>    RqlCompileError: Cannot use r.row in nested queries.  Use functions instead in:
C>    r.table("foodb").get("user-foo1").update({friend_counts: r.db("foodb").table("friends")("friend2_id").count(r.row("id"))})

The reason for this error is because RethinkDB doesn't know which query to base
`r.row` on? Is it the main query, table **users**, or sub query, table
**friends**. Luckily, We can use an anynoymous function to solve this. Function
allows access to current document but it solve problem of `r.row` because it 
clearly binds to a sequence.

### Expression

Let's get some basic knowledge then we will come back to the previous example.

Beside passing an object into `update` command, we can also pass an expression or 
a function which returns an object. RethinkDB will evaluate it, get the object
result and use that value for `update` command. It comes in useful when you 
have some logic on your document related to the updating. In case of function,
the function receive first parameter is the current document.

With previous example, we can re-write using function:

    r.db('foodb').table('users')
      .get('user-foo1')
      .update(function (user) {
        return {
          friend_counts: r.db('foodb').table('friends')('friend2_id').count(user("id"))
        }
      })

Then, we got an error:

    RqlRuntimeError: Could not prove function deterministic.  Maybe you want to use the non_atomic flag? in:
    r.db("foodb").table("users").get("user-foo1").update(function(var_58) { return {friend_counts: r.db("foodb").table("friends")("friend2_id").count(var_58("id"))}; })

Well, this is because the updating isn't **atomic**[^atomic1][^atomic2]

[^atomic1]: http://rethinkdb.com/docs/architecture/#query-execution
[^atomic2]: http://rethinkdb.com/docs/architecture/#query-execution

Atomic update mean that an update to a document either succesfull,
or fail and no change is made. Such as we update two fields of a document,
first field update is succesfull but second field fail to update. Atomic
guarantees that both of fields will be sucesfully updated to new value.
Any update happen on a single JSON document is guaranteed to be atomic city.
So what is a non-atomic update? Non atomic update is setting value to
result of executing JavaScript code, random values, and values obtained as a
result of a subquery

T> non-atomic updates
T>
T> A good way to remember what is non-atomic update is that they are usually
T> value which cannot be predicate such as random values, result of other query

To run a non-atomic update, we have to clearly tell RethinkDB that with `nonAtomic`
option:

    r.db('foodb').table('users')
      .get('user-foo1')
      .update(function (user) {
        return {
          friend_counts: r.db('foodb').table('friends')('friend2_id').count(user("id"))
        }
      }, {nonAtomic: true})
    //=>
    {
    "deleted": 0 ,
    "errors": 0 ,
    "inserted": 0 ,
    "replaced": 1 ,
    "skipped": 0 ,
    "unchanged": 0
    }

We can verify the update really succesfully:

    r.db('foodb').table('users')
      .get('user-foo1')('friend_counts')
    //=>
    2

So we can see that in RethinkDB, we have to opt-in to use some features. Later
on, we know that we have to manually passing an index name to use it. That may
a little bit verbose at first. But that helps you understand query and let you
know what you are doing here.

Updating with function is really similar to passing object to update function.
We have to return an JSON object with key-value similar to the JSON document
that we pass directly to **update** command.

We can name the parameter of function to whatever. The name isn't important. It
is just like a callback function, in what RethinkDB will pass the real value
of current document to it when invoke that function. What we can do with `r.row`
we can mostly do with that parameter. Such as getting value of field with.
Let's change **user** to *u* and see if it works:

    r.db('foodb').table('users')
      .get('user-foo1')
      .update(function (u) {
        return {
          friend_counts: r.db('foodb').table('friends')('friend2_id').count(u("id"))
        }
      }, {nonAtomic: true})

Let's do one more complex example. If an users has more than 10 **friends**, we
set a field **social_status** to *extrovert*, otherwise, it's *introvert*.

    r.db('foodb').table('users')
      .get('user-foo1')
      .update(function (u) {
        return {
          social_status: r.branch(u('friend_counts').gt(10), 'extrovert', 'introvert')
        }
      }, {nonAtomic: true})

Here we are using a new command `r.branch`. It's like an `IF` in MySQL. If the
first argument is TRUE, the second argument is the return value, otherwise the
third argument. We use `user('friend_count')` to get value of `friend_count`
as you know. We calling `gt` command on it. `gt` means greater, it returns
**TRUE** if the value is greater than what we pass to `gt`.

When using a function, the parameter pass into function will be the current
visited document. Therefore, you can use many document manipulation command in
it such as: **pluck**, **without**, **merge**, **append**, **prepend**. Just remember this, so
you know what you can do with that parameter.

#### Expr

`expr` is a normal function but I think they are important and will help us
achive many crazy things so I cover them here.

What `expr` does is tranform a native object from host language into ReQL
object. For example, if a RethinkDb funciton can be call on array or sequence,
we cannot write something like this: `[e1, e2].nth(2)`, RethinkDB will throw an
error on [e1, e2]

    ["e1","e2"].nth is not a function

What we have to do is somehow convert the array that we write in native
language into RethinkDB data type. To do that, we simply wrap them in `expr`

A real example when I'm writing this book is I want to randomize generate
faked data for `users` table on `gender` field. I do this with:

    r.db("foodb").table("users")
      .update({
        gender: r.expr(['m', 'f']).nth(r.random(0, 2))
      }, {nonAtomic: true})

It means that for every document of `users` table, I want to set their gender to
either `m` or `f` randomly. I create a two element array `[m, f]`, turn them
into ReQL object with `expr`, so that I can call `nth` on them, passing a random
number of either 0 or 1.

let's try a more complex example to generate some data. For every users, we
generate a list of *eatenfoods* name randomly by select all food name, and use
`sample(number)` command to select a number of random element.

    r.db("foodb").table("users")
      .update({
          eatenfoods: r.db("foodb").table("foods").sample(r.random(0,
10)).getField('name')
        },
        {nonAtomic: true}
      )

We have two random here. First, we random how many of number of food we want to
get from 0 to 10 by calling `r.random` with a range. Then we use `sample`
command to get us that number of random document. 

Now, we have `eatenfoods` field. Let's say we want to create a field contains
the foods that an user has eaten, and his or her most favourite foods (first element
in `favfoods` field)

    r.db("foodb").table("users")
      .update({
        eateanorlike : r.add(r.row("eatenfoods"), [r.row("favfoods").nth(0)])
      }, {nonAtomic: true})

By combining ReQL expressions, looking at RethinkDB API and find approriate
function, we can achieve what we want. In the above example, we know we want to
concat two arrays from `eatenfoods` and first item of `favfoods`. We used `r.add`. We have
to wrap `r.row("favfoods").nth(0)` in `[]` because `nth()` return a document,
where as `r.add` expects array, so we wrap it in `[]`.

We also didn't have an `age` field on those **users** table. Let's generate some fake
data for it so we can play around later. Here we randomize `age` between 8 and 90.

    r.db("foodb").table("users")
      .update({
        age : r.random(8, 90)
      }, {nonAtomic: true})
    #=>
    {
    "deleted": 0 ,
    "errors": 0 ,
    "inserted": 0 ,
    "replaced": 152 ,
    "skipped": 0 ,
    "unchanged": 0
    }

By using function and/or expression, we can update document in a complex way.
Just carefully look up RethinkDB API, we can find the function we want. If not,
we can probably whip some logic inside function.

### Return Values

Sometimes, it can be useful to get back the updated document. This way you can
verify the result, without issuing a sub sequent `get` command. We just need to
set `returnChanges` flag to true in option parameter of `update` command. Same
example:

    r.db('foodb').table('users')
      .get('user-foo1')
      .update(function (user) {
        return {
          _social_status: r.branch(user('friend_counts').gt(10), 'extrovert', 'introvert')
        }
      }, {nonAtomic: true, returnChanges: true})
    //=>
    {
        "changes": [{
            "new_val": {
                "_social_status": "introvert",
                "address": [{
                    "city": "Cuppertino",
                    "country": "USA",
                    "number": 11,
                    "state": "CA",
                    "ste": 880,
                    "street": "Infinite Loop"
                }, {
                    "city": "Hue",
                    "country": "Vietnam",
                    "number": "131",
                    "street": "Tran Phu"
                }],
                "age": 13,
                "another_address_field": [{
                    "city": "Hue",
                    "country": "Vietnam",
                    "number": "131",
                    "street": "Tran Phu"
                }],
                "friend_counts": 2,
                "gender": "f",
                "id": "user-foo1",
                "name": "foo",
                "social": {
                    "facebook": "kureikain3",
                    "github": "kureikain",
                    "twitter": "kureikain2"
                },
                "social_status": "introvert"
            },
            "old_val": {
                "address": [{
                    "city": "Cuppertino",
                    "country": "USA",
                    "number": 11,
                    "state": "CA",
                    "ste": 880,
                    "street": "Infinite Loop"
                }, {
                    "city": "Hue",
                    "country": "Vietnam",
                    "number": "131",
                    "street": "Tran Phu"
                }],
                "age": 13,
                "another_address_field": [{
                    "city": "Hue",
                    "country": "Vietnam",
                    "number": "131",
                    "street": "Tran Phu"
                }],
                "friend_counts": 2,
                "gender": "f",
                "id": "user-foo1",
                "name": "foo",
                "social": {
                    "facebook": "kureikain3",
                    "github": "kureikain",
                    "twitter": "kureikain2"
                },
                "social_status": "introvert"
            }
        }],
        "deleted": 0,
        "errors": 0,
        "inserted": 0,
        "replaced": 1,
        "skipped": 0,
        "unchanged": 0
    }

The old value and value are returned in key `old_val` and `new_val`
correspondingly.

As you see, you started to mess up our data. It's ok, let lean some command
that destroy data. Let's meep **replace** command.

## Replace

Let's see how we remove a field from a document. Assume we want to remove
field `eateanorlike`.

To remove one or many fields from document, we cannot use `update` anymore.
We can set a field to `null` value(null, nil depends on your language) to make
it becomes null. But they key is still in the document with a `null` value.
In other words, `update` lets us overwrite fields, but don't remove them.
That's why we have another command for removing fields. The `replace` command
replaces entire document with new document.

    r.db("foodb").table("users").replace(r.row.without('eateanorlike'))

Here we use `r.row` to get current documents, then calling `without` to remove
the field.

`without` accepts a list of argument and will remove those fields with that
name from document. Such as:

    r.db('foodb').table('users')
      .get('user-foo1')
      .without("address", "another_fieldaddress_field", "social")
    //=>
    {
      "_social_status":  "introvert" ,
      "age": 13 ,
      "friend_counts": 2 ,
      "gender":  "f" ,
      "id":  "user-foo1" ,
      "name":  "foo" ,
      "social_status":  "introvert"
    }

`without` can also remove nested field. Such as remove `country` field from
**address**

    r.db('foodb').table('users')
      .get('user-foo1')
      .without({address: "country"}, "another_address_field", "social")
    //=>
    {
        "_social_status": "introvert",
        "address": [{
            "city": "Cuppertino",
            "number": 11,
            "state": "CA",
            "ste": 880,
            "street": "Infinite Loop"
        }, {
            "city": "Hue",
            "number": "131",
            "street": "Tran Phu"
        }],
        "age": 13,
        "friend_counts": 2,
        "gender": "f",
        "id": "user-foo1",
        "name": "foo",
        "social_status": "introvert"
    }

We can use nested style to denote the field that we want to remove. If we want
to remove many fields, wrap them in array.

Example, we remove all fields of **address** except **country**:

    r.db('foodb').table('users')
      .get('user-foo1')
      .without({address: ["number","state", "city", "ste", "street"]}, "another_address_field", "social")
    //=>
    {
        "_social_status": "introvert",
        "address": [{
            "country": "USA"
        }, {
            "country": "Vietnam"
        }],
        "age": 13,
        "friend_counts": 2,
        "gender": "f",
        "id": "user-foo1",
        "name": "foo",
        "social_status": "introvert"
    }

Note that, We can replace an entirely new document, however, the primary key
cannot be changed. It has to be same with the current primary key. An attempt to change
the primary key will caused an error **Primary key `id` cannot be changed**

    r.db("foodb").table("users")
      .get("user-foo1")
      .replace({id: 1})

We will got an error:

    {
        "deleted": 0,
        "errors": 1,
        "first_error": "Primary key `id` cannot be changed (`{
        "_social_status": "introvert",
        "address": [{
            "city": "Cuppertino",
            "country": "USA",
            "number": 11,
            "state": "CA",
            "ste": 880,
            "street": "Infinite Loop"
        }, {
            "city": "Hue",
            "country": "Vietnam",
            "number": "131",
            "street": "Tran Phu"
        }],
        "age": 13,
        "friend_counts": 2,
        "gender": "f",
        "id": "user-foo1",
        "name": "foo",
        "social": {
            "facebook": "kureikain3",
            "github": "kureikain",
            "twitter": "kureikain2"
        },
        "social_status": "introvert"
    }
    ` -> ` {
        "id": 1
    }
    `)." ,
    "inserted": 0 ,
    "replaced": 0 ,
    "skipped": 0 ,
    "unchanged": 0
    }

`replace` also has a useful feature is that if it detects a document with same
primary key, it's replace the document, otherwise it can insert a new document.
When using this feature, we have to make sure to pass primary key in replace
document:

    r.table('test').get('primary_key').replace({
      id: 'primary_key',
      field: 'value'
    })

replace or insert is helpful when we want to ensure that the document will be
existed in database. In above query, we first call `get` to find document, if a
document is founed, then `replace` is executed and replace it. Because the
primary keys are same, only other **fields** will be replaced. If `get` return
NULL, replace will insert the whole document.

One thing to remember is that we cannot replace the primary key of document.
Because changing primary key is actually simply removing old one and insert
a new document. Let's learn about removing data.

## Delete

Delete is similar to `update` or `replace`. We select a sequence, and call
`delete` command on them.

This will delete a single document, by using primary to select it with `get` 
then calling `delete` on that single document.

    r.db("foodb").table("users")
      .get("user-foo2")
      .delete()

We cal also clear a whole table or a selection. Let's play with it via some
temporary table:

    r.db("foodb").tableCreate("test1")
    r.db("foodb").table("test1").insert({field: 'foo', field2: 'bar'})
    r.db("foodb").table("test1").insert({field: 'foo2', field2: 'bar2'})
    r.db("foodb").table("test1").insert({age: 10, name: 'abc'})
    r.db("foodb").table("test1").insert({age: 12, name: 'abc2'})

Let's remove user who are under age of 11.

    r.db("foodb")
      .table("test1")
      .filter(r.row('age').lt(11))
      .delete()

We using `r.row` to get current document, then get the **age** field value, and
call `lt` to do compare **less than**.

Basically with any selection, we can call `delete` on them, it goes over and
remove data.

So you can alredy guess command to delete all table:

    r.db("foodb").table("test1").delete()

Like `update`, `delete` method accepts an optional object with:

  * durability: 'hard' or 'soft'. default is 'hard'
  * returnChanges: 'false' or true. default is false. When setting to true,
    a changes array will be return with **old_val** and **new_value** if the document is succesfully to remove

And finally, before we move on. Let's remove use `user-foo1` since we mess with
it a bit

    r.db("foodb").table("users")
      .get("user-foo1")
      .delete()

## Sync

As you known in the previous chapter, with value of `durability` as 'soft' the
write isn't guarantees to be written to the permanent storage. So after doing a
bunch of those `soft durability`, you may want to say `Hey, I am done all task,
let's make sure you write those change` you can call sync

Using JavaScript driver:

  r.table('t').sync().run(connection, function () {
    console.log('Syncing is done. All data is safe now')
  })

**sync** command will block until all previous write to the table are persited.
With that being said, `sync` will be only called on **table**.

It's good idea to do a bunch of `soft durability` and call `sync` at the end
to ensure data persitent and still avoid blocking during executing of other
logic.

# Wrap up

Some important concept you should remember:

  * atomicity
  * synca
  * multiple insert
  * change primary field
  * update with function
  * using nested field style to remove nested field
